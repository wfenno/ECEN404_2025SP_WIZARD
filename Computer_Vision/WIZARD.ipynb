{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb8f017-90b7-403b-bbe6-46121e8e906c",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8539227-1a26-420e-bd29-1073f571c590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Installing all dependencies needed\n",
    "!pip install labelme opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a847ad0-0fc8-4dcd-a2da-61e4f6258a96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Displays everything installed \n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d29522-ae44-46e2-a84a-1a04b35bcbd7",
   "metadata": {},
   "source": [
    "# Label Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230252d-f83e-42b7-8ffc-87164c5eaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for labeling the images in the dataset\n",
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4826ac2-52e5-47b2-9e84-46952b0f1b3d",
   "metadata": {},
   "source": [
    "# Construct Image Loading Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc438fdd-3558-47ef-9ba1-7c7a58417a24",
   "metadata": {},
   "source": [
    "## Import Dependencies & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425630b9-53a7-45d1-9928-fb950e989f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow for building and running the deep learning model\n",
    "import tensorflow as tf\n",
    "# Import OpenCV for image and video processing\n",
    "import cv2\n",
    "# Import time module to measure time intervals\n",
    "import time\n",
    "# Import json to handle JSON files\n",
    "import json\n",
    "# Import os to interact with the operating system \n",
    "import os\n",
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "# Import pyplot from matplotlib for plotting and displaying images\n",
    "from matplotlib import pyplot as plt\n",
    "# Import PIL Image for additional image manipulation options\n",
    "from PIL import Image\n",
    "# Import datetime to get the current date and time \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f54e9-8653-42be-b7b1-f3d0ccd1c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all physical GPU devices recognized by TensorFlow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# Loop through each GPU detected to configure its memory growth setting\n",
    "for gpu in gpus:\n",
    "    # Enable memory growth for the GPU to allocate memory as needed, preventing TensorFlow from using all GPU memory at once\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5a5a9-3360-458f-be0f-a877413032e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputs availability of GPU\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80673c5e-f431-4a42-807f-4e97372fccbb",
   "metadata": {},
   "source": [
    "## Load Image into Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246fabf-d177-4d8d-9013-f0bb50693f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads an image and resizes the tensor to 224x224x3\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT = 224  \n",
    "IMG_WIDTH = 224   \n",
    "\n",
    "def load_image(x):\n",
    "    # Read the image file as a byte string\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    # Decode the byte string into an image tensor with 3 channels (RGB)\n",
    "    img = tf.image.decode_image(byte_img, channels=3)\n",
    "    # Setting shape of the image for 3 channels\n",
    "    img.set_shape([None, None, 3])  \n",
    "    # Resize the image to (224x224)\n",
    "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])  \n",
    "    \n",
    "    # Returning preprocessed image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d3821-17e9-48c0-aebf-6e01eefdb1d2",
   "metadata": {},
   "source": [
    "#  Image Augmentation for Images & Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5667c-2dcd-4fb6-be75-fc140aa1d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the following augmentations to the dataset\n",
    "\n",
    "\"\"\"\n",
    " - HorizontalFlip: Flips the image horizontally.\n",
    " - RandomBrightnessContrast: Randomly adjusts the brightness and contrast.\n",
    " - RandomGamma: Randomly applies gamma correction.\n",
    " - RGBShift: Randomly shifts the RGB channels.\n",
    " - VerticalFlip: Flips the image vertically.\n",
    " \"\"\"\n",
    "\n",
    "#Importing image augmentation library\n",
    "import albumentations as alb\n",
    "augmentor = alb.Compose([alb.HorizontalFlip(p=0.5),\n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2),\n",
    "                         alb.RGBShift(p=0.2),\n",
    "                         alb.VerticalFlip(p=0.5)],\n",
    "                         bbox_params = alb.BboxParams(format = 'albumentations', label_fields = ['class_labels']))\n",
    "\n",
    "#albumentations format = normalized[x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8111a3-9186-41a0-880e-15eda6dc66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting example image for testing labels\n",
    "img = cv2.imread(os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Data\\\\Train\\\\Images', 'baby_soybean_jpeg (4).jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e41c72-4fa5-4888-9542-b1eccd0fdc89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extracting dimensions (height and width)\n",
    "height, width, _ = img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec962c79-720a-4ecb-afb4-49c9999f2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting label\n",
    "with open(os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Data\\\\Train\\\\Labels', 'baby_soybean_jpeg (4).json'), 'r') as f:\n",
    "    # Load the JSON content as a dictionary, so it can be accessed as 'label'\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc75465-f7fe-42c0-a571-a65589bcc367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Bounding Box Coordinates:\\n\", label['shapes'][0]['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d57a7-296d-4d78-a699-0e8530692be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list of four zeros to store bounding box coordinates [x1, y1, x2, y2]\n",
    "coords = [0, 0, 0, 0]\n",
    "\n",
    "# Assign the x-coordinate of the first point to the first index of 'coords'\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "# Assign the y-coordinate of the first point to the second index of 'coords'\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "# Assign the x-coordinate of the second point to the third index of 'coords'\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "# Assign the y-coordinate of the second point to the fourth index of 'coords'\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247570a4-ad10-4831-92d0-1b62b615277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords\n",
    "#raw pascal_voc format = [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38481bfe-cb3f-43fc-b838-373313d3c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the bounding box coordinates \n",
    "coords = list(np.divide(coords, [width, height, width, height]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ad707-e144-4fc8-8ffc-2b40ae7a63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords\n",
    "#albumentations format = normalized[x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f508a-5f6f-4ba7-96a3-24f6366e7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmenting specific crop image\n",
    "augmented = augmentor(image = img, bboxes = [coords], class_labels = ['Crop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f03ad7-2a28-4146-b4fc-32bbdeeaac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a rectangle on the augmented image using bounding box coordinates\n",
    "cv2.rectangle(\n",
    "    augmented['image'],  # The image to draw the rectangle on (augmented image)\n",
    "    tuple(np.multiply(augmented['bboxes'][0][:2], [width, height]).astype(int)),  # Top-left corner coordinates (scaled to original image size)\n",
    "    tuple(np.multiply(augmented['bboxes'][0][2:], [width, height]).astype(int)),  # Bottom-right corner coordinates (scaled to original image size)\n",
    "    (0, 255, 0),  # Color of the rectangle (green in BGR format)\n",
    "    2)  # Thickness of the rectangle's border (2 pixels)\n",
    "\n",
    "# Displaying the augmented image with the bounding box using matplotlib\n",
    "plt.imshow(augmented['image'])  # This will show the image with the drawn rectangle in the Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdad888-176d-4700-a248-d3f683abca7a",
   "metadata": {},
   "source": [
    "# Build Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2637101-7ca0-4e6a-a346-43ad3fa80ee6",
   "metadata": {},
   "source": [
    "## Create Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19024dd-b5df-416f-ab1a-39f2a9baea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation pipeline using Albumentations\n",
    "\n",
    "# Iterate through partitions\n",
    "for partition in ['Train', 'Test', 'Val']: \n",
    "    # Define paths for images, labels, and augmented data\n",
    "    image_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Datav2', partition, 'Images')\n",
    "    label_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Datav2', partition, 'Labels')\n",
    "    aug_image_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6', partition, 'Images')\n",
    "    aug_label_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6', partition, 'Labels')\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(aug_image_dir, exist_ok=True)\n",
    "    os.makedirs(aug_label_dir, exist_ok=True)\n",
    "\n",
    "    # Process each image in the current partition\n",
    "    for image in os.listdir(image_dir):\n",
    "        img = cv2.imread(os.path.join(image_dir, image))\n",
    "\n",
    "        # Get the image dimensions\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Initialize empty list for bounding boxes and class labels\n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        \n",
    "        # Define path to the corresponding label file\n",
    "        label_path = os.path.join(label_dir, f'{image.split(\".\")[0]}.json')\n",
    "\n",
    "        # Read labels if the label file exists\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label_data = json.load(f)\n",
    "\n",
    "            # Iterate through all shapes in the label file (multiple bboxes)\n",
    "            for shape in label_data['shapes']:\n",
    "                # Normalize bounding box coordinates based on image dimensions\n",
    "                coords = [\n",
    "                    shape['points'][0][0] / width,  # x_min\n",
    "                    shape['points'][0][1] / height,  # y_min\n",
    "                    shape['points'][1][0] / width,  # x_max\n",
    "                    shape['points'][1][1] / height   # y_max\n",
    "                ]\n",
    "                bboxes.append(coords)\n",
    "\n",
    "                # Handle missing 'class' key\n",
    "                if 'class' in shape:\n",
    "                    class_labels.append(shape['class'])\n",
    "                else:\n",
    "                    # Assign labels based on filename if 'class' is missing\n",
    "                    if 'soybean' in image.lower():\n",
    "                        class_labels.append(0)  # Assign 0 for crop\n",
    "                    elif 'weed' in image.lower() or 'rrpw' in image.lower():\n",
    "                        class_labels.append(1)  # Assign 1 for weed\n",
    "\n",
    "        try:\n",
    "            # Set the number of augmentations based on class label\n",
    "            if 0 in class_labels:  #crop\n",
    "                augmentations_count = 22\n",
    "            elif 1 in class_labels:  #weed\n",
    "                augmentations_count = 10\n",
    "            else:\n",
    "                augmentations_count = 0  # No augmentation if class not recognized\n",
    "\n",
    "            # Perform augmentation\n",
    "            for x in range(augmentations_count):\n",
    "                augmented = augmentor(image=img, bboxes=bboxes, class_labels=class_labels)\n",
    "\n",
    "                # Save augmented image\n",
    "                augmented_image_path = os.path.join(aug_image_dir, f'{image.split(\".\")[0]}.{x}.jpg')\n",
    "                cv2.imwrite(augmented_image_path, augmented['image'])\n",
    "\n",
    "                # Prepare and save the augmented bounding box annotations\n",
    "                augmented_annotations = []\n",
    "                for aug_bbox, aug_class in zip(augmented['bboxes'], augmented['class_labels']):\n",
    "                    annotation = {\n",
    "                        \"image\": f'{image.split(\".\")[0]}.{x}.jpg', # Augmented image filename\n",
    "                        \"class\": aug_class, # Class label\n",
    "                        \"bbox\": aug_bbox  # Coordinates in albumentations format\n",
    "                    }\n",
    "                    augmented_annotations.append(annotation)\n",
    "\n",
    "                # Save annotations to JSON file\n",
    "                augmented_label_path = os.path.join(aug_label_dir, f'{image.split(\".\")[0]}.{x}.json')\n",
    "                with open(augmented_label_path, 'w') as f:\n",
    "                    json.dump(augmented_annotations, f, indent=4)\n",
    "\n",
    "        except Exception as e: # Handle errors during processing and display the filename and error\n",
    "            print(f\"Error processing {image}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c58d7-14d1-4a91-84c9-0125a7737daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_background_images():\n",
    "    # Iterate through partitions\n",
    "    for partition in ['Train', 'Test', 'Val']: \n",
    "        # Define paths for images, labels, and augmented data\n",
    "        image_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Background', partition, 'Images')\n",
    "        aug_image_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6', partition, 'Images')\n",
    "        aug_label_dir = os.path.join('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6', partition, 'Labels')\n",
    "\n",
    "        # Set the augmentation count based on the partition\n",
    "        augmentation_count = 54 if partition == 'Train' else 50\n",
    "\n",
    "        # Ensure output directories exist\n",
    "        os.makedirs(aug_image_dir, exist_ok=True)\n",
    "        os.makedirs(aug_label_dir, exist_ok=True)\n",
    "\n",
    "        for image in os.listdir(image_dir):\n",
    "            img = cv2.imread(os.path.join(image_dir, image))\n",
    "\n",
    "            # Perform augmentation the specified number of times\n",
    "            for x in range(augmentation_count):\n",
    "                augmented = augmentor(image=img, bboxes=[], class_labels=[])\n",
    "\n",
    "                # Save augmented image\n",
    "                augmented_image_path = os.path.join(aug_image_dir, f'{image.split(\".\")[0]}.{x}.jpg')\n",
    "                cv2.imwrite(augmented_image_path, augmented['image'])\n",
    "\n",
    "                # Save augmented labels\n",
    "                augmented_annotations = [{\n",
    "                    \"image\": f'{image.split(\".\")[0]}.{x}.jpg',\n",
    "                    \"class\": 2,  # Class 2 for background\n",
    "                    \"bbox\": [0, 0, 0, 0]  # Use [0, 0, 0, 0] for background\n",
    "                }]\n",
    "\n",
    "                # Save to JSON file in the Labels folder\n",
    "                augmented_label_path = os.path.join(aug_label_dir, f'{image.split(\".\")[0]}.{x}.json')\n",
    "                with open(augmented_label_path, 'w') as f:\n",
    "                    json.dump(augmented_annotations, f, indent=4)\n",
    "\n",
    "# Run the augmentation function\n",
    "augment_background_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a301d-0949-4298-a620-a7a59c047d7f",
   "metadata": {},
   "source": [
    "## Load Augmented Images to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d835db1-eab9-4ef4-a41a-db8e449675cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts each image file path into a resized image tensor of \n",
    "# shape (224,224,3) with normalized pixel values between (0,1)\n",
    "train_images = tf.data.Dataset.list_files('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Train\\\\Images\\\\*.jpg', shuffle=False)\n",
    "# Load each image file and decode it into a tensor with RGB channels\n",
    "train_images = train_images.map(load_image)\n",
    "# Explicitly resize each image tensor to (224, 224)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (224,224)))\n",
    "# Normalize pixel values to the range [0, 1] by dividing by 255\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea360e-0a8e-4813-97e3-9522a27578c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts each image file path into a resized image tensor of \n",
    "# shape (224,224,3) with normalized pixel values between (0,1)\n",
    "test_images = tf.data.Dataset.list_files('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Test\\\\Images\\\\*.jpg', shuffle=False)\n",
    "# Load each image file and decode it into a tensor with RGB channels\n",
    "test_images = test_images.map(load_image)\n",
    "# Explicitly resize each image tensor to (224, 224)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (224,224)))\n",
    "# Normalize pixel values to the range [0, 1] by dividing by 255\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b3148-8793-4447-b244-4a6bc8e15f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts each image file path into a resized image tensor of \n",
    "# shape (224,224,3) with normalized pixel values between (0,1)\n",
    "val_images = tf.data.Dataset.list_files('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Val\\\\Images\\\\*.jpg', shuffle=False)\n",
    "# Load each image file and decode it into a tensor with RGB channels\n",
    "val_images = val_images.map(load_image)\n",
    "# Explicitly resize each image tensor to (224, 224)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (224,224)))\n",
    "# Normalize pixel values to the range [0, 1] by dividing by 255\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7e58b-7a16-4627-8ceb-716751d0df55",
   "metadata": {},
   "source": [
    "# Prepare Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ddd0d5-ad91-41e9-b96f-b84e4888410d",
   "metadata": {},
   "source": [
    "## Build Label Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968fdfb3-079b-41af-b837-8d1049f29abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_classes = 7 \n",
    "\n",
    "\"\"\"\n",
    "Max number of annotations from a label in the dataset. \n",
    "Padding the rest of labels to meet the maximum number of annotations so that \n",
    "the number is consistant rather than dealing with variable annotations across each label.\n",
    "\"\"\"\n",
    "\n",
    "def load_labels(label_path):\n",
    "    try:\n",
    "        # Read the JSON file\n",
    "        label_data = tf.io.read_file(label_path)\n",
    "        labels = json.loads(label_data.numpy().decode('utf-8'))  # Decode bytes to string\n",
    "\n",
    "        # Extract classes and bounding boxes\n",
    "        classes = [item['class'] for item in labels]\n",
    "        bboxes = [item['bbox'] for item in labels]\n",
    "        \n",
    "        # Pad classes list and convert to tensor\n",
    "        class_tensor = tf.convert_to_tensor(classes + [255] * (max_classes - len(classes)), dtype=tf.uint8)\n",
    "\n",
    "        # Pad bounding boxes and convert to tensor\n",
    "        if len(bboxes) == 0: # Background\n",
    "            # If there are no bounding boxes, fill with padding\n",
    "            bbox_tensor = tf.convert_to_tensor([[0, 0, 0, 0]] * max_classes, dtype=tf.float32)\n",
    "        else:\n",
    "            # Pad bounding boxes\n",
    "            bbox_tensor = tf.convert_to_tensor(bboxes + [[0, 0, 0, 0]] * (max_classes - len(bboxes)), dtype=tf.float32)\n",
    "        \n",
    "        return class_tensor, bbox_tensor\n",
    "\n",
    "    # Error handling\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {label_path} was not found.\")\n",
    "        return None, None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {label_path} could not be decoded.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b49c52-681b-48e0-b027-24056a7d113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to test and display annotations\n",
    "def test_load_labels():\n",
    "    # Load labels\n",
    "    classes, bboxes = load_labels('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Test\\\\Labels\\\\baby_soybean_jpeg (3).0.json')\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Class Labels:\", classes.numpy())  # Convert tensor to numpy for easy reading\n",
    "    print(\"Bounding Boxes: \\n\", bboxes.numpy())\n",
    "\n",
    "# Run the test\n",
    "test_load_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294e4d5-872b-4358-b94f-9d7d8e6b7161",
   "metadata": {},
   "source": [
    "## Load Labels to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61242d7a-6df9-4071-97e3-8bdf8b408ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label files from the directory as a tf.data.Dataset\n",
    "train_labels = tf.data.Dataset.list_files('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Train\\\\Labels\\\\*.json', shuffle=False)\n",
    "\n",
    "# Map over each label file and process them using the `load_labels` function\n",
    "# Use tf.py_function and set shapes for consistent output\n",
    "train_labels = train_labels.map(\n",
    "    lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float32]), # Apply `load_labels` function\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Ensure the output shape of each processed label matches the expected dimensions\n",
    "train_labels = train_labels.map(lambda class_tensor, bbox_tensor: (\n",
    "    tf.ensure_shape(class_tensor, [max_classes]), #Shape [7]\n",
    "    tf.ensure_shape(bbox_tensor, [max_classes, 4]))) #Shape [7,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20409d81-a9bf-4f6d-ab58-a6e4e748ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label files from the directory as a tf.data.Dataset\n",
    "test_labels = tf.data.Dataset.list_files('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Test\\\\Labels\\\\*.json', shuffle=False)\n",
    "\n",
    "# Map over each label file and process them using the `load_labels` function\n",
    "# Use tf.py_function and set shapes for consistent output\n",
    "test_labels = test_labels.map(\n",
    "    lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float32]), # Apply `load_labels` function\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Ensure the output shape of each processed label matches the expected dimensions\n",
    "test_labels = test_labels.map(lambda class_tensor, bbox_tensor: (\n",
    "    tf.ensure_shape(class_tensor, [max_classes]), #Shape [7]\n",
    "    tf.ensure_shape(bbox_tensor, [max_classes, 4]))) #Shape [7,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6afd89-0333-438b-8f2e-6e5ccce00467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label files from the directory as a tf.data.Dataset\n",
    "val_labels = tf.data.Dataset.list_files('C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\Aug_Datav6\\\\Val\\\\Labels\\\\*.json', shuffle=False)\n",
    "\n",
    "# Map over each label file and process them using the `load_labels` function\n",
    "# Use tf.py_function to call the custom Python function for each file\n",
    "val_labels = val_labels.map(\n",
    "    lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float32]), # Apply `load_labels` function\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Ensure the output shape of each processed label matches the expected dimensions\n",
    "val_labels = val_labels.map(lambda class_tensor, bbox_tensor: (\n",
    "    tf.ensure_shape(class_tensor, [max_classes]), #Shape [7]\n",
    "    tf.ensure_shape(bbox_tensor, [max_classes, 4]))) #Shape [7,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853a879-e363-47ef-bfe4-0bf1e11dc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print a small batch from the dataset\n",
    "def print_dataset_example(dataset, num_examples):\n",
    "    # Iterate over the dataset and print the specified number of examples\n",
    "    for classes, bboxes in dataset.take(num_examples):\n",
    "        print(\"Classes:\", classes.numpy())\n",
    "        print(\"Bounding Boxes:\", bboxes.numpy())\n",
    "\n",
    "# Print one example from the `test_labels` dataset\n",
    "print_dataset_example(test_labels, num_examples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee9145-548f-44e4-8a03-0d0b2195eb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the train_labels dataset to a NumPy iterator and fetch the next example\n",
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e57c95-9b6b-496a-bb80-f89073687b0d",
   "metadata": {},
   "source": [
    "# Combine Labels & Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5402a-9ec2-4e8a-ae26-28d86adecbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing length of train, test, and val images and labels\n",
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6071f4-c453-4cb6-a7b7-fd55255d00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train_images and train_labels datasets into a single dataset\n",
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "\n",
    "# Shuffle the dataset with a buffer size of 6000\n",
    "# Batch the dataset into batches of size 32\n",
    "# Prefetch the data in the background while the model is training \n",
    "train = train.shuffle(buffer_size=6000)\n",
    "train = train.batch(32)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddaa71-03b6-4595-9816-e07cc7ac1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the test_images and test_labels datasets into a single dataset\n",
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "\n",
    "# Batch the dataset into batches of size 32 \n",
    "# Prefetch the data in the background while the model is being evaluated \n",
    "test = test.batch(32)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bafd08-071a-4ff0-a0a6-812480c89856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the val_images and val_labels datasets into a single dataset\n",
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "\n",
    "# Batch the dataset into batches of size 32\n",
    "# Prefetch the data in the background while the model is being validated \n",
    "val = val.batch(32)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc627a7-6ff7-4d38-af60-6951afa25698",
   "metadata": {},
   "source": [
    "## View Images &  Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818ed51-c3a1-402e-9917-ab1272d94628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for the training dataset\n",
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ceae26-4552-4529-8085-66eb9ec7bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the next batch\n",
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28633d-d042-45ce-89e2-e6d3bb708704",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = res[0]  # Image batch\n",
    "classes = res[1][0]  # Padded classes\n",
    "bboxes = res[1][1]  # Padded bounding boxes\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 20))\n",
    "\n",
    "for ax, img, bbox, cls in zip(axes, images, bboxes, classes):\n",
    "    # Convert image to BGR format (OpenCV uses BGR)\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for box, label in zip(bbox, cls):\n",
    "        if label == 255:  # Skip padded boxes\n",
    "            continue\n",
    "\n",
    "        # Rescale, color, & draw bboxes (green for class 0 (crop), red for class 1 (weed))\n",
    "        x1, y1, x2, y2 = (box * [img.shape[1], img.shape[0], img.shape[1], img.shape[0]]).astype(int)  \n",
    "        color = (0, 255, 0) if label == 0 else (255, 0, 0) \n",
    "        cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 2)\n",
    "    \n",
    "    # Display the image with bounding boxes on the subplot\n",
    "    ax.imshow(img_bgr)\n",
    "    ax.axis('on')\n",
    "    \n",
    "# Display the plot with all images and bounding boxes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017f269-6285-432b-a721-fdd04562cb3a",
   "metadata": {},
   "source": [
    "# Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb235312-2d01-440b-b77b-b23a47d218dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential  # Import Model and Sequential classes for creating models\n",
    "from tensorflow.keras import layers, models  # Import layers and models for building and defining neural network structures\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Dropout, GlobalMaxPooling2D, Flatten, AveragePooling2D, BatchNormalization, Reshape, Lambda, Activation  \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # Import callbacks for model training (e.g., stop early, save model, reduce learning rate)\n",
    "from tensorflow.keras.regularizers import l1_l2, l2  # Import regularizers to prevent overfitting by adding penalties on model parameters\n",
    "from tensorflow.keras.applications import VGG16  # Import the pre-trained VGG16 model (used for transfer learning)\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy  # Import accuracy metric for multi-class classification tasks\n",
    "from tensorflow.keras.callbacks import Callback  # Import Callback class for creating custom callbacks during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27973f3b-1c4e-421a-8a6b-4d3865596b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing VGG16 without the fully connected layer as model backbone\n",
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57948dc7-7b59-4985-a903-f90b289f9915",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shows vgg architecture\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee27632-ec19-4a9a-8e8f-de7384ef991f",
   "metadata": {},
   "source": [
    "## Build Instance of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7bce2-f844-4c99-80c9-4d465c1bc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIZARDv15 Model \n",
    "def build_model(num_boxes=7, num_classes=3): \n",
    "    input_layer = Input(shape=(224, 224, 3))  # Define the input layer with shape 224x224x3 (RGB image)\n",
    "\n",
    "    # Feature Extraction using VGG16 with pre-trained weights\n",
    "    vgg_base = VGG16(weights='imagenet', include_top=False, input_tensor=input_layer)  # Use VGG16 as a base model for feature extraction, without the fully connected layers\n",
    "    vgg_base.trainable = False  # Freeze the VGG16 layers to prevent updates during training\n",
    "    \n",
    "    vgg_output = vgg_base.output  # Capture the output of VGG16 feature extractor\n",
    "\n",
    "    # Optionally unfreeze last few layers for fine-tuning\n",
    "    for layer in vgg_base.layers[-4:]:  # Select the last 4 layers of VGG16 to be trainable\n",
    "        layer.trainable = True  # Enable training on the selected layers for fine-tuning\n",
    "    \n",
    "    # Global Feature Pooling\n",
    "    f1 = GlobalMaxPooling2D()(vgg_output)  # Apply global max pooling to reduce the spatial dimensions to a single vector\n",
    "    \n",
    "    # Classification Head\n",
    "    class_head = Dense(1024, activation='relu', kernel_regularizer=l1_l2(l1=0.00095, l2=0.0095))(f1)  # Dense layer with L1/L2 regularization for class prediction\n",
    "    class_head = BatchNormalization()(class_head)  # Normalize the activations to improve training stability\n",
    "    class_head = Dropout(0.5)(class_head)  # Dropout layer to prevent overfitting in the classification head\n",
    "    class_head = Dense(num_boxes * num_classes, activation='linear')(class_head)  # Dense layer to predict class scores for each bounding box\n",
    "    class_head = Reshape((num_boxes, num_classes))(class_head)  # Reshape to (num_boxes, num_classes) for each bounding box\n",
    "    class_head = Lambda(lambda x: tf.nn.softmax(x, axis=-1), name='classification_output')(class_head)  # Apply softmax activation to get probabilities for each class\n",
    "    \n",
    "    # Localization Head\n",
    "    regress_head = Dense(1024, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.001))(f1)  # Dense layer with L1/L2 regularization for bounding box coordinates\n",
    "    regress_head = BatchNormalization()(regress_head)  # Normalize the activations in the localization head\n",
    "    regress_head = Dropout(0.2)(regress_head)  # Dropout layer to reduce overfitting in the localization head\n",
    "    regress_head = Dense(num_boxes * 4, activation='sigmoid')(regress_head)  # Dense layer for bounding box coordinates (x, y, w, h), scaled between 0 and 1\n",
    "    regress_head = Reshape((num_boxes, 4), name='localization_output')(regress_head)  # Reshape to (num_boxes, 4) for bounding box output\n",
    "    \n",
    "    # Define the Model with Dual Outputs\n",
    "    tracker = Model(inputs=input_layer, outputs=[class_head, regress_head])  # Create the model with classification and localization heads as outputs\n",
    "    return tracker  # Return the complete model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d16cf0-0049-46e5-bc4f-776426ab6260",
   "metadata": {},
   "source": [
    "## Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905b4d5-490d-490b-acdf-52815cbf9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "tracker = build_model(num_boxes=7, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3aee88-f993-42ee-9d7d-3b28a462c5b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shows WIZARD model architeture\n",
    "tracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd366a46-cd91-4704-8a37-8965f78673b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images - X, Labels - y\n",
    "#Gets next batch\n",
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732a188-3885-4767-bbe9-73cfab1bd9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows shape of Images\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eaecf2-4ce2-4a9f-a6e0-22841146f0ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y[0]\n",
    "# Batch class array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d1a7f-f724-4f40-9e16-07596d0d6598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y[1]\n",
    "# Batch bounding box array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df07147-3956-4c28-86a1-2c3a04380ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction class label and bbox coordinates\n",
    "classes, coords = tracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f26b8-9670-42a4-bbd3-5e5dad8ba3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes, coords\n",
    "#classes - probability of crop, probability of weed, probability of background\n",
    "#coords - predicted bbox coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353564be-00f2-4c87-b601-ee8444cb18d6",
   "metadata": {},
   "source": [
    "# Determine Losses & Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b276e4-f754-42c9-8742-5b2e7949265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM (Adaptive Moment Estimation) optimizer algorithm\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c3e8d-8052-4391-a4c8-da1fd5416f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat, delta=1.0):\n",
    "    \"\"\"\n",
    "    Computes the Huber loss bewteen predicted and true bboxes\n",
    "    while ignoring padded entries that don't represent actual bboxes.\n",
    "\n",
    "    Args:\n",
    "        y_true: Tensor of true values (ground truth), shape (batch_size, num_boxes, num_classes or 4).\n",
    "        yhat: Tensor of predicted values, shape (batch_size, num_boxes, num_classes or 4).\n",
    "        delta: Threshold for switching between L2 and L1 loss.\n",
    "\n",
    "    Returns:\n",
    "        Scalar tensor representing the average Huber loss over the batch.\n",
    "    \"\"\"\n",
    "    # Create a mask for valid boxes (non-padded)\n",
    "    mask = tf.reduce_any(tf.abs(y_true) > 1e-6, axis=-1)  # Adjust based on your padding representation\n",
    "    mask = tf.cast(mask, dtype=tf.float32)  # Shape: (batch_size, num_boxes)\n",
    "\n",
    "    # Calculate the absolute differences\n",
    "    diff = y_true - yhat  # Shape: (batch_size, num_boxes, num_classes or 4)\n",
    "    abs_diff = tf.abs(diff)  # Shape: (batch_size, num_boxes, num_classes or 4)\n",
    "\n",
    "    # Calculate Huber loss based on the absolute differences\n",
    "    quadratic_loss = 0.5 * tf.square(diff)  # Shape: (batch_size, num_boxes, num_classes or 4)\n",
    "    linear_loss = delta * (abs_diff - 0.5 * delta)  # Shape: (batch_size, num_boxes, num_classes or 4)\n",
    "\n",
    "    # Use tf.where to select the appropriate loss based on the delta condition\n",
    "    loss_per_box = tf.where(abs_diff <= delta, quadratic_loss, linear_loss)  # Shape: (batch_size, num_boxes, num_classes or 4)\n",
    "\n",
    "    # Sum the losses for each bounding box (sum across the last axis)\n",
    "    loss_per_box_sum = tf.reduce_sum(loss_per_box, axis=-1)  # Shape: (batch_size, num_boxes)\n",
    "\n",
    "    # Apply the mask to ignore padded boxes\n",
    "    masked_loss = loss_per_box_sum * mask # Shape: (batch_size, num_boxes)\n",
    "\n",
    "    # Compute the total loss by summing over all boxes and normalizing by the number of valid boxes\n",
    "    total_loss = tf.reduce_sum(masked_loss) / (tf.reduce_sum(mask) + 1e-8)  # Scalar\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e8c64-c7c1-4c7a-ab24-7c301f3fd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CategoricalCrossentropy: used for multi-class classification tasks\n",
    "# from_logits=False: outputs are probabilities (from softmax activation)\n",
    "\n",
    "classloss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab9d3a-f080-4bd9-9419-5872aeb9557f",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2b7cc-32dc-4fc9-ab75-a120390bf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(y_classes, num_classes=3):\n",
    "    \"\"\"\n",
    "    Converts class labels to one-hot encoding and sets padded boxes to [0, 0, 1] (Background).\n",
    "\n",
    "    Args:\n",
    "        y_classes: Tensor of shape (batch_size, num_boxes), values {0, 1, 2, 255}\n",
    "        num_classes: Number of classes (including Background)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, num_boxes, num_classes)\n",
    "    \"\"\"\n",
    "    # Convert to float for manipulation\n",
    "    y_classes = tf.cast(y_classes, tf.float32)\n",
    "    \n",
    "    # Set padded boxes to Background class (2)\n",
    "    y_classes = tf.where(tf.equal(y_classes, 255), \n",
    "                         tf.ones_like(y_classes) * 2,  # Background class index\n",
    "                         y_classes)\n",
    "    \n",
    "    # Check for invalid class labels using tf.assert\n",
    "    valid_classes = tf.logical_and(tf.greater_equal(y_classes, 0), tf.less(y_classes, num_classes))\n",
    "    # Use tf.debugging.assert for error checking in TensorFlow's graph mode\n",
    "    tf.debugging.assert_equal(tf.reduce_all(valid_classes), True, \n",
    "                              message=\"Input contains invalid class labels. Must be in range [0, {}]\".format(num_classes - 1))\n",
    "        \n",
    "    # Cast to int for one-hot encoding\n",
    "    y_classes = tf.cast(y_classes, tf.int32)\n",
    "    \n",
    "    # One-hot encode the class labels\n",
    "    y_classes = tf.one_hot(y_classes, depth=num_classes)  # Shape: (batch_size, num_boxes, num_classes)\n",
    "    \n",
    "    return y_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63cd6a-d5b1-43fc-aaee-0920d2ca41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(Model):\n",
    "    def __init__(self, tracker, **kwargs): # Initializing model \"tracker\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = tracker # Store the model\n",
    "        self.categorical_accuracy = CategoricalAccuracy(name='accuracy') # Metric for accuracy\n",
    "\n",
    "    # Compiling model with instance variables used during training and evaluation\n",
    "    def compile(self, opt, classloss, localization_loss, **kwargs): \n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss # Store the classification loss\n",
    "        self.lloss = localization_loss # Store the localization loss\n",
    "        self.opt = opt # Store the ADAM optimizer\n",
    "\n",
    "    def train_step(self, batch, **kwargs): \n",
    "        X, y = batch  # X = image, y[0] = classes, y[1] = bboxes\n",
    "        \n",
    "        # Track gradients with tf.GradientTape\n",
    "        with tf.GradientTape() as tape: \n",
    "            classes_pred, coords_pred = self.model(X, training=True) # Forward pass for predictions\n",
    "            \n",
    "            # Prepare labels: [1, 0, 0] for Crop, [0, 1, 0] for Weed, [0, 0, 1] for Background\n",
    "            labels = prepare_labels(y[0], num_classes=3)  # Shape: (batch_size, num_boxes, 3)\n",
    "\n",
    "            # Calculates class loss, regress loss, and total loss\n",
    "            class_loss_value = self.closs(labels, classes_pred)\n",
    "            localization_loss_value = self.lloss(y[1], coords_pred)\n",
    "            total_loss = localization_loss_value + class_loss_value\n",
    "\n",
    "        # Compute gradients and apply them using optimizer\n",
    "        grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        # Update categorical accuracy metric\n",
    "        self.categorical_accuracy.update_state(labels, classes_pred)\n",
    "        \n",
    "        # Return loss values and metrics\n",
    "        return {\n",
    "            \"total_loss\": total_loss, \n",
    "            \"class_loss\": class_loss_value, \n",
    "            \"regress_loss\": localization_loss_value, \n",
    "            \"accuracy\": self.categorical_accuracy.result()\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, **kwargs): # Predicts class and bboxes\n",
    "        X, y = batch\n",
    "        \n",
    "        # Get predictions for both class and coordinates (bounding boxes)\n",
    "        classes_pred, coords_pred = self.model(X, training=False)\n",
    "\n",
    "        # Prepare labels: [1, 0, 0] for Crop, [0, 1, 0] for Weed, [0, 0, 1] for Background\n",
    "        labels = prepare_labels(y[0], num_classes=3)  # Shape: (batch_size, num_boxes, 3)\n",
    "\n",
    "        # Calculates class loss, regress loss, and total loss\n",
    "        class_loss_value = self.closs(labels, classes_pred)\n",
    "        localization_loss_value = self.lloss(y[1], coords_pred)\n",
    "        total_loss = localization_loss_value + class_loss_value\n",
    "        \n",
    "        # Update categorical accuracy metric\n",
    "        self.categorical_accuracy.update_state(labels, classes_pred)\n",
    "\n",
    "        # Return loss values and metrics\n",
    "        return {\n",
    "            \"total_loss\": total_loss, \n",
    "            \"class_loss\": class_loss_value, \n",
    "            \"regress_loss\": localization_loss_value, \n",
    "            \"accuracy\": self.categorical_accuracy.result()\n",
    "        }\n",
    "\n",
    "    def call(self, X, **kwargs): \n",
    "        \"\"\"\n",
    "        Call the underlying model to get predictions for input X.\n",
    "        Args:\n",
    "            X: Input data (images).\n",
    "        Returns:\n",
    "            Model predictions (class and coordinates).\n",
    "        \"\"\"\n",
    "        return self.model(X, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19954093-5fbc-48a2-8d05-3a660a492b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instance of Tracker class model\n",
    "model = Tracker(tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14e29d-7715-4101-84af-31436d9508b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the model with the optimizer and loss functions\n",
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b290b0cb-6dbd-4f50-9fbc-062aa10aa34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard toop used for visualizing metrics like loss and accuracy\n",
    "logdir='logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea0dd3-5c52-4c56-91e3-c9bd5e8bd088",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "loss_lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_total_loss',        # Monitor validation loss\n",
    "    factor=0.3,\n",
    "    patience=5,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26ba24-eadd-4152-9b36-8be4c39afc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_total_loss',   # Metric to monitor\n",
    "    patience=8,                 # Number of epochs with no improvement after which training will stop\n",
    "    restore_best_weights=True   # Restores model weights from the epoch with the best value of the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018360f-62e7-41a2-93ae-30028feaece3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model with training data and validation data for a specified number of epochs\n",
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794206b5-4cf1-4c30-a335-6b9506d33193",
   "metadata": {},
   "source": [
    "## Plot for Losses & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a3d9c-a46a-46f4-8154-176076ab9134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a figure with 4 subplots (columns) for visualizing each metric\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(30, 5))\n",
    "\n",
    "# Plot for Total Loss\n",
    "# Plot training total loss with 'teal' color and label it as 'loss'\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "# Plot validation total loss with 'orange' color and label it as 'val loss'\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "# Set the title for the first subplot to 'Total Loss'\n",
    "ax[0].title.set_text('Total Loss')\n",
    "# Display legend for the first plot\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot for Classification Loss\n",
    "# Plot training classification loss with 'teal' color and label it as 'class loss'\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "# Plot validation classification loss with 'orange' color and label it as 'val class loss'\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "# Set the title for the second subplot to 'Classification Loss'\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "# Display legend for the second plot\n",
    "ax[1].legend()\n",
    "\n",
    "# Plot for Regression Loss\n",
    "# Plot training regression loss with 'teal' color and label it as 'regress loss'\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "# Plot validation regression loss with 'orange' color and label it as 'val regress loss'\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "# Set the title for the third subplot to 'Regression Loss'\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "# Display legend for the third plot\n",
    "ax[2].legend()\n",
    "\n",
    "# Plot for Accuracy\n",
    "# Plot training accuracy with 'teal' color and label it as 'accuracy'\n",
    "ax[3].plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "# Plot validation accuracy with 'orange' color and label it as 'val accuracy'\n",
    "ax[3].plot(hist.history['val_accuracy'], color='orange', label='val accuracy')\n",
    "# Set the title for the fourth subplot to 'Accuracy'\n",
    "ax[3].title.set_text('Accuracy')\n",
    "# Display legend for the fourth plot\n",
    "ax[3].legend()\n",
    "\n",
    "# Show all the plots in a single figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33166cc-7330-4e32-ba2c-7f6eb7cbfbe9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to smooth out data points using a moving average\n",
    "def smooth_curve(points, window_size=3):\n",
    "    smoothed_points = []  # Initialize a list to store smoothed points\n",
    "    for i in range(len(points)):\n",
    "        # Determine the start of the moving window, ensuring we don't go below index 0\n",
    "        start = max(0, i - window_size + 1)\n",
    "        # Extract the window of points to average\n",
    "        window = points[start:i + 1]\n",
    "        # Calculate the average of the current window and append to smoothed points\n",
    "        smoothed_points.append(sum(window) / len(window))\n",
    "    return smoothed_points  # Return the list of smoothed points\n",
    "\n",
    "# Create a figure with 4 subplots for visualizing each metric\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(30, 5))\n",
    "\n",
    "# Plot for Total Loss\n",
    "# Smooth and plot training total loss with 'teal' color\n",
    "ax[0].plot(smooth_curve(hist.history['total_loss']), color='teal', label='loss')\n",
    "# Smooth and plot validation total loss with 'orange' color\n",
    "ax[0].plot(smooth_curve(hist.history['val_total_loss']), color='orange', label='val loss')\n",
    "# Set the title for the first subplot to 'Total Loss'\n",
    "ax[0].title.set_text('Total Loss')\n",
    "# Display legend for the first plot\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot for Classification Loss\n",
    "# Smooth and plot training classification loss with 'teal' color\n",
    "ax[1].plot(smooth_curve(hist.history['class_loss']), color='teal', label='class loss')\n",
    "# Smooth and plot validation classification loss with 'orange' color\n",
    "ax[1].plot(smooth_curve(hist.history['val_class_loss']), color='orange', label='val class loss')\n",
    "# Set the title for the second subplot to 'Classification Loss'\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "# Display legend for the second plot\n",
    "ax[1].legend()\n",
    "\n",
    "# Plot for Regression Loss\n",
    "# Smooth and plot training regression loss with 'teal' color\n",
    "ax[2].plot(smooth_curve(hist.history['regress_loss']), color='teal', label='regress loss')\n",
    "# Smooth and plot validation regression loss with 'orange' color\n",
    "ax[2].plot(smooth_curve(hist.history['val_regress_loss']), color='orange', label='val regress loss')\n",
    "# Set the title for the third subplot to 'Regression Loss'\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "# Display legend for the third plot\n",
    "ax[2].legend()\n",
    "\n",
    "# Plot for Accuracy\n",
    "# Smooth and plot training accuracy with 'teal' color\n",
    "ax[3].plot(smooth_curve(hist.history['accuracy']), color='teal', label='accuracy')\n",
    "# Smooth and plot validation accuracy with 'orange' color\n",
    "ax[3].plot(smooth_curve(hist.history['val_accuracy']), color='orange', label='val accuracy')\n",
    "# Set the title for the fourth subplot to 'Accuracy'\n",
    "ax[3].title.set_text('Accuracy')\n",
    "# Display legend for the fourth plot\n",
    "ax[3].legend()\n",
    "\n",
    "# Show all the plots in a single figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4240d96a-1278-4ccb-85ca-1aa176916ddc",
   "metadata": {},
   "source": [
    "# Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f977c81-b6f9-4ab1-b8da-38cf18f99f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_with_background(yhat, test_sample, iou_threshold=0.5, score_threshold=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots the predictions with Non-Maximum Suppression applied and handles Background class.\n",
    "    yhat: Tuple of (class_predictions, bbox_predictions)\n",
    "    test_sample: Tuple of (images, labels)\n",
    "    iou_threshold: IoU threshold for NMS\n",
    "    score_threshold: Confidence threshold to filter boxes\n",
    "    \"\"\"\n",
    "    \n",
    "    class_preds, bbox_preds = yhat  # Unpacks the predictions into class probabilities and bounding box coordinates\n",
    "    images = test_sample[0]  # Extracts images from the test sample tuple\n",
    "    num_images = 4  # Set the number of images to display\n",
    "    fig, ax = plt.subplots(ncols=num_images, figsize=(20, 20))  # Creates subplots to display images\n",
    "    \n",
    "    for img_idx in range(num_images):  # Loops through the number of images\n",
    "        # Copy the current image to add bounding boxes\n",
    "        sample_image = images[img_idx].copy()  # Copies the image for modification\n",
    "        class_probs = class_preds[img_idx]  # Extracts the class probabilities for the current image\n",
    "        bbox_coords = bbox_preds[img_idx]   # Extracts the bounding box coordinates for the current image\n",
    "        label_classes = test_sample[1][0][img_idx]  # Extracts the labels for the current image\n",
    "\n",
    "        boxes = []  # List to hold the bounding box coordinates\n",
    "        scores = []  # List to hold the confidence scores for each box\n",
    "        classes = []  # List to hold the class labels for each box\n",
    "        \n",
    "        for box_idx in range(7):  # Loops through each of the 7 boxes in the prediction\n",
    "            if label_classes[box_idx] == 255:  # If the class label is 255, it's a padded box (no object)\n",
    "                # Assign zero probability and coordinates to padded boxes\n",
    "                class_probs[box_idx] = [0, 0, 1]  # Set class probabilities to background\n",
    "                bbox_coords[box_idx] = [0, 0, 0, 0]  # Set bounding box coordinates to (0, 0, 0, 0)\n",
    "                continue  # Skip plotting padded boxes\n",
    "\n",
    "            # Extract probabilities for each class (Crop, Weed, Background)\n",
    "            prob_crop, prob_weed, prob_background = class_probs[box_idx]\n",
    "            \n",
    "            # Determine the class with the highest probability\n",
    "            if prob_background > score_threshold:  \n",
    "                continue  # Skip for background case\n",
    "\n",
    "            # Handle Crop class (only display confidence score, no bounding box)\n",
    "            if prob_crop > prob_weed and prob_crop > score_threshold:  \n",
    "                class_name = \"Crop\"  # Set the class name to Crop\n",
    "                confidence = prob_crop  # Set the confidence to the crop probability\n",
    "                color = (0, 255, 0)  # Set label color to green\n",
    "                # Display confidence score at bottom-left corner of the image\n",
    "                bottom_left_corner = (10, sample_image.shape[0] - 10)  # Coordinates for the bottom-left corner\n",
    "                cv2.putText(sample_image, f'{class_name}: {confidence:.2f}', bottom_left_corner, \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)  # Add text to the image\n",
    "                continue  # Skip drawing a bounding box for Crop\n",
    "\n",
    "            # Handle Weed class (display bounding box with score)\n",
    "            elif prob_weed > prob_crop and prob_weed > score_threshold: \n",
    "                class_name = \"Weed\"  # Set the class name to Weed\n",
    "                confidence = prob_weed  # Set the confidence to the weed probability\n",
    "                color = (255, 0, 0)  # Set the bounding box color to red\n",
    "                \n",
    "                # Append bounding box coordinates, confidence score, and class to lists for selected boxes\n",
    "                boxes.append(bbox_coords[box_idx])\n",
    "                scores.append(confidence)\n",
    "                classes.append(class_name)\n",
    "        \n",
    "        if boxes:  # If there are any boxes to process\n",
    "            boxes = np.array(boxes)  # Convert bounding boxes to NumPy array\n",
    "            scores = np.array(scores)  # Convert scores to NumPy array\n",
    "            classes = np.array(classes)  # Convert classes to NumPy array\n",
    "            \n",
    "            # Apply Non-Maximum Suppression (NMS) to reduce overlapping boxes\n",
    "            selected_indices = tf.image.non_max_suppression(boxes, scores, max_output_size=5, iou_threshold=iou_threshold) \n",
    "            selected_boxes = tf.gather(boxes, selected_indices).numpy()  # Get selected bounding boxes\n",
    "            selected_scores = tf.gather(scores, selected_indices).numpy()  # Get selected scores\n",
    "            selected_classes = classes[selected_indices.numpy()]  # Get selected classes\n",
    "\n",
    "            for i in range(len(selected_boxes)):  # Loop through the selected boxes after NMS\n",
    "                box = selected_boxes[i]  # Get the coordinates of the selected bounding box\n",
    "                class_name = selected_classes[i]  # Get the class name for the selected box\n",
    "                score = selected_scores[i]  # Get the confidence score for the selected box\n",
    "                color = (0, 255, 0) if class_name == \"Crop\" else (255, 0, 0)  # Set the color to green for Crop or red for Weed\n",
    "                \n",
    "                # Scale bounding boxes to the image size (224x224)\n",
    "                start_point = tuple(np.multiply(box[:2], [224, 224]).astype(int))  # Calculate the top-left corner\n",
    "                end_point = tuple(np.multiply(box[2:], [224, 224]).astype(int))  # Calculate the bottom-right corner\n",
    "                \n",
    "                # Draw the bounding box on the image (only for Weed)\n",
    "                cv2.rectangle(sample_image, start_point, end_point, color, 2)  # Draw the rectangle on the image\n",
    "                # Add a label with the class name and confidence score\n",
    "                cv2.putText(sample_image, f'{class_name}: {score:.2f}', start_point, \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)  # Display the label on the image\n",
    "        \n",
    "        ax[img_idx].imshow(sample_image)  # Display the image in the corresponding subplot\n",
    "        ax[img_idx].axis('on')  # Turn on the axis for the subplot\n",
    "        \n",
    "    plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28931961-abf9-4d47-bd5f-ac6b3102f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for the test dataset to manually retrieve batches of data\n",
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd615490-5226-4c55-9a34-69358dab5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the next batch of data from the test iterator\n",
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7455e6-7c06-4bb7-9192-8651022c2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the batch into features (X_test) and labels (y_test)\n",
    "# Use the model to make predictions on the test batch (X_test)\n",
    "X_test, y_test = test_sample\n",
    "yhat = tracker.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5b6aa-ac77-4662-ae52-f9ec6f1817fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test predictions on current model\n",
    "plot_predictions_with_background(yhat, test_sample, iou_threshold=0.5, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b6ba9-1e64-4337-99b7-8aaf9cbc0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicitions with older model\n",
    "model_path = 'WIZARDv15.h5'  \n",
    "tracker = load_model(model_path)\n",
    "\n",
    "# Get the test data iterator once\n",
    "test_data = test.as_numpy_iterator()\n",
    "\n",
    "# Function to manually test a batch\n",
    "def test_batch(batch_idx=0, iou_threshold=0.5, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Manually test a batch from the test dataset and plot the predictions.\n",
    "    batch_idx: Index of the batch to test (0, 1, 2, etc.)\n",
    "    \"\"\"\n",
    "    # Fetch the batch\n",
    "    for _ in range(batch_idx):\n",
    "        test_data.next()  # Skip to the desired batch\n",
    "    \n",
    "    # Get the next batch (or the batch at batch_idx)\n",
    "    test_sample = test_data.next()\n",
    "    X_test, y_test = test_sample\n",
    "    \n",
    "    # Run model prediction\n",
    "    yhat = tracker.predict(X_test)\n",
    "    \n",
    "    # Plot predictions for this batch\n",
    "    plot_predictions_with_background(yhat, test_sample, iou_threshold, score_threshold)\n",
    "\n",
    "# To test a specific batch, just call:\n",
    "test_batch(batch_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01a7d1-27d6-4673-be8a-7cd1f6fcc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in any saved model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9dc6f8-639a-45e9-98e4-1d3889b2fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the model version\n",
    "tracker.save('WIZARDv15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e12c1e-67ab-4d1f-ae04-9082e8dbf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in the model for use\n",
    "tracker = load_model('WIZARDv15.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e043c0-b771-4641-b192-b1e0995c8285",
   "metadata": {},
   "source": [
    "## Real Time Detection Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccaee8-63b8-40de-8a71-6ae371ad0da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = 'WIZARDv15.h5'  \n",
    "tracker = load_model(model_path)  # Load the trained model using Keras' load_model function\n",
    "\n",
    "# Define colors for bounding boxes\n",
    "CROP_COLOR = (0, 255, 0)  # Green color for crop (only for text)\n",
    "WEED_COLOR = (0, 0, 255)  # Red color for weeds\n",
    "\n",
    "def preprocess_frame(frame, input_size=(224, 224)):\n",
    "    \"\"\"Preprocess the frame for model input.\"\"\"\n",
    "    frame_resized = cv2.resize(frame, input_size)  # Resize the frame to the input size expected by the model (224x224)\n",
    "    frame_normalized = frame_resized / 255.0  # Normalize pixel values to the range [0, 1]\n",
    "    return np.expand_dims(frame_normalized, axis=0)  # Add batch dimension (expand to shape (1, 224, 224, 3))\n",
    "\n",
    "def draw_bounding_boxes(frame, yhat, iou_threshold=0.5, score_threshold=0.75):\n",
    "    \"\"\"Draw bounding boxes on the frame using model predictions.\"\"\"\n",
    "    class_preds, bbox_preds = yhat  # Extract class predictions and bounding box predictions from the model's output\n",
    "    sample_image = frame.copy()  # Create a copy of the frame to draw on\n",
    "\n",
    "    class_probs = class_preds[0]  # Shape: (7, 3) - Class probabilities for 7 boxes\n",
    "    bbox_coords = bbox_preds[0]   # Shape: (7, 4) - Bounding box coordinates for 7 boxes\n",
    "\n",
    "    boxes = []  # List to store valid bounding boxes\n",
    "    scores = []  # List to store confidence scores for bounding boxes\n",
    "    classes = []  # List to store class labels for bounding boxes\n",
    "\n",
    "    # Iterate through each bounding box prediction\n",
    "    for box_idx in range(7):  # Assuming there are 7 bounding boxes predicted per image\n",
    "        # Extract the probabilities for each class (Crop, Weed, Background)\n",
    "        prob_crop, prob_weed, prob_background = class_probs[box_idx]\n",
    "\n",
    "        # For Crop: do not display bounding box, only show confidence score at bottom left\n",
    "        if prob_crop > prob_weed and prob_crop > score_threshold:\n",
    "            class_name = \"Crop\"  # Label the object as \"Crop\"\n",
    "            confidence = prob_crop  # Set confidence score to crop's probability\n",
    "            color = CROP_COLOR  # Set color to green for crop\n",
    "            continue  # Skip adding bounding box for Crop, handled separately\n",
    "\n",
    "        # For Weed: display bounding box with confidence score\n",
    "        elif prob_weed > prob_crop and prob_weed > score_threshold:\n",
    "            class_name = \"Weed\"  # Label the object as \"Weed\"\n",
    "            confidence = prob_weed  # Set confidence score to weed's probability\n",
    "            color = WEED_COLOR  # Set color to red for weed\n",
    "            boxes.append(bbox_coords[box_idx])  # Add bounding box coordinates to the list\n",
    "            scores.append(confidence)  # Add confidence score to the list\n",
    "            classes.append(class_name)  # Add class label to the list\n",
    "\n",
    "        else:\n",
    "            continue  # Skip low-confidence or background detections\n",
    "\n",
    "    if boxes:  # If there are valid boxes to display\n",
    "        boxes = np.array(boxes)  # Convert list of boxes to a numpy array\n",
    "        scores = np.array(scores)  # Convert list of scores to a numpy array\n",
    "        classes = np.array(classes)  # Convert list of classes to a numpy array\n",
    "\n",
    "        # Apply Non-Maximum Suppression to remove overlapping boxes\n",
    "        selected_indices = tf.image.non_max_suppression(boxes, scores, max_output_size=5, iou_threshold=iou_threshold)\n",
    "        selected_boxes = tf.gather(boxes, selected_indices).numpy()  # Get boxes that were selected after NMS\n",
    "        selected_scores = tf.gather(scores, selected_indices).numpy()  # Get scores for selected boxes\n",
    "        selected_classes = classes[selected_indices.numpy()]  # Get class labels for selected boxes\n",
    "\n",
    "        # Draw each box that was kept after NMS\n",
    "        for i in range(len(selected_boxes)):\n",
    "            box = selected_boxes[i]  # Get coordinates of the bounding box\n",
    "            class_name = selected_classes[i]  # Get the class name of the object\n",
    "            score = selected_scores[i]  # Get confidence score\n",
    "            color = WEED_COLOR  # Set color to red for weed\n",
    "\n",
    "            # Scale bounding boxes back to original frame size\n",
    "            frame_height, frame_width = frame.shape[:2]  # Get the dimensions of the original frame\n",
    "            start_point = tuple(np.multiply(box[:2], [frame_width, frame_height]).astype(int))  # Top-left corner\n",
    "            end_point = tuple(np.multiply(box[2:], [frame_width, frame_height]).astype(int))  # Bottom-right corner\n",
    "\n",
    "            # Draw the bounding box on the frame for Weed\n",
    "            if class_name == \"Weed\":  # Only draw box for \"Weed\" class\n",
    "                cv2.rectangle(frame, start_point, end_point, color, 2)  # Draw rectangle\n",
    "                cv2.putText(frame, f'{class_name}: {score:.2f}', start_point,  # Add text showing class and score\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # Display Crop confidence score at the bottom left of the frame (no bounding box)\n",
    "    for box_idx in range(7):\n",
    "        prob_crop = class_probs[box_idx][0]  # Get crop class probability\n",
    "        if prob_crop > score_threshold:  # If crop probability exceeds the threshold\n",
    "            cv2.putText(frame, f'Crop: {prob_crop:.2f}', (10, frame.shape[0] - 10),  # Display crop score at bottom left\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, CROP_COLOR, 2, cv2.LINE_AA)\n",
    "            break  # Only display one Crop confidence score (for the first detected crop)\n",
    "\n",
    "    return frame  # Return the frame with bounding boxes and labels\n",
    "\n",
    "def real_time_detection(model):\n",
    "    \"\"\"Function to run real-time object detection with webcam using the preloaded `tracker` model.\"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # Open the webcam (0 is the default camera)\n",
    "\n",
    "    # Check if the webcam is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")  # If there's an error, print and return\n",
    "        return\n",
    "\n",
    "    # Set up the video writer to save the output\n",
    "    now = datetime.now().strftime('%Y%m%d_%H%M%S')  # Get current timestamp for file naming\n",
    "    output_filename = f'C:\\\\Users\\\\wfenn\\\\ECEN_403\\\\RealTimeObjectDetection\\\\Real_Time_Tests\\\\{model_path.split(\".\")[0]}_{now}.mp4'  # File path for output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video codec (MP4)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Get the frame width of the video\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get the frame height of the video\n",
    "    out = cv2.VideoWriter(output_filename, fourcc, 5.0, (frame_width, frame_height))  # Initialize video writer\n",
    "\n",
    "    # Loop to continuously capture frames from the webcam\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Capture a frame from the webcam\n",
    "\n",
    "        if not ret:  # If frame capture fails\n",
    "            print(\"Error: Failed to capture image.\")  # Print error message and break the loop\n",
    "            break\n",
    "\n",
    "        # Un-mirror the frame by flipping it horizontally\n",
    "        #frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Preprocess the frame for the model\n",
    "        preprocessed_frame = preprocess_frame(frame)  # Preprocess the captured frame\n",
    "        # Get predictions from the model\n",
    "        yhat = tracker.predict(preprocessed_frame)  # Get model's output\n",
    "        # Draw the bounding boxes based on the predictions\n",
    "        output_frame = draw_bounding_boxes(frame, yhat)  # Get the frame with bounding boxes drawn\n",
    "\n",
    "        # Write the frame to the output file\n",
    "        out.write(output_frame)  # Save the frame with bounding boxes to the output video file\n",
    "\n",
    "        # Display the frame with bounding boxes\n",
    "        cv2.imshow('Real-Time Detection', output_frame)  # Show the frame in a window\n",
    "\n",
    "        if cv2.waitKey(50) & 0xFF == ord('q'):  # Wait for 'q' key to exit\n",
    "            break\n",
    "\n",
    "    cap.release()  # Release the webcam\n",
    "    out.release()  # Release the video writer\n",
    "    cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "\n",
    "# Start real-time detection\n",
    "real_time_detection(tracker) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ObjectDetection",
   "language": "python",
   "name": "objectdetection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
